# VNPT AI Credentials (required for 'vnpt' provider)
VNPT_ACCESS_TOKEN=YOUR_VNPT_ACCESS_TOKEN
VNPT_TOKEN_ID=YOUR_VNPT_TOKEN_ID
VNPT_TOKEN_KEY=YOUR_VNPT_TOKEN_KEY

# General Provider Configuration
CHAT_PROVIDER=vnpt                                # Supported: vnpt, ollama, openai
EMBEDDING_PROVIDER=vnpt                           # Supported: vnpt, huggingface

# Model Configuration
MODEL_NAME=vnptai-hackathon-small               # For VNPT Chat: vnptai-hackathon-small, vnptai-hackathon-large
                                                # For Ollama/OpenAI: e.g., gpt-3.5-turbo, gemma2:270m
HUGGINGFACE_EMBEDDING_MODEL=all-MiniLM-L6-v2    # Optional: if EMBEDDING_PROVIDER is huggingface

# --- Provider-Specific Configuration ---
# OLLAMA_BASE=http://localhost:11434            # Uncomment and set for Ollama provider
# OPENAI_API_KEY=YOUR_OPENAI_API_KEY            # Uncomment and set for OpenAI provider

# Performance and Rate Limiting
CONCURRENT_REQUESTS=2                           # Number of concurrent LLM requests (default: 2)
SLEEP_TIME=90                                   # Seconds to sleep between requests to respect API quotas (default: 90 for 40 req/hr)

# Logging
LOG_LEVEL=INFO                                  # Logging level (e.g., DEBUG, INFO, WARNING, ERROR)

# Retrieval-Augmented Generation (RAG) Configuration
RAG_ENABLED=false                               # Enable/disable RAG (true/false)
USE_PRE_RETRIEVED_CONTEXT=false                 # Use pre-retrieved context from input file (true/false)
FAISS_INDEX_PATH=knowledge_base/faiss_index.bin # Path to the FAISS index file
TEXT_CHUNKS_PATH=knowledge_base/text_chunks.json # Path to the text chunks JSON file
BM25_INDEX_PATH=knowledge_base/bm25_index.pkl   # Path to the BM25 index file

# Hybrid Search Configuration (requires RAG_ENABLED=true)
HYBRID_SEARCH_ENABLED=false                     # Enable/disable hybrid search (true/false)
SEMANTIC_WEIGHT=0.5                             # Weight for semantic search (FAISS) in hybrid search (0.0 to 1.0)
KEYWORD_WEIGHT=0.5                              # Weight for keyword search (BM25) in hybrid search (0.0 to 1.0)

# Re-ranking Configuration (requires RAG_ENABLED=true)
RERANK_ENABLED=false                            # Enable/disable re-ranking of retrieved documents (true/false)
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2 # Cross-encoder model to use for re-ranking
RETRIEVAL_TOP_K=10                              # Number of initial documents to retrieve for re-ranking
TOP_K_RAG=3                                     # Number of top documents to use after re-ranking for the LLM prompt